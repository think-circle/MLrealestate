{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B><H2> HOUSE PRICE ESTIMATOR </H2></B><H5>TENSORFLOW REGRESSION SEQUENTIAL DNN MODEL</H5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import docker\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#TENSORFLOW API\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "# Feature Engineering\n",
    "from tensorflow import feature_column as fc\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "# TF Dataset for input pipeline\n",
    "import tensorflow_datasets as tfds\n",
    "# If using GPU use the below config\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "\n",
    "# VISUALISATION API\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Seaborn is a Python data visualization library based on matplotlib.\n",
    "\n",
    "# Import train_test_split function from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Used to calculate stats such as Z score, standard deviation etc.\n",
    "from scipy import stats\n",
    "# Used to calculate MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "city_list = [\"Melbourne\", \"Sydney\",\"Brisbane\",\"Perth\",\"Adelaide\",\"Hobart\"]\n",
    "city = city_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H5> READ THE DATA INTO PANDAS DATAFRAME.</H5> Remove any NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from CSV into pandas DF\n",
    "df= pd.read_csv(f\"data/{city}/{city}_area.csv\")\n",
    "# Check if there any NULL or NaN values\n",
    "df.isnull().sum()\n",
    "# Drop any NA values\n",
    "df = df.dropna(how='any',axis=0)\n",
    "# Check that there is no longer Null values\n",
    "df.isnull().sum()\n",
    "# Look at DF to get a feel of data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> <B> EXPLORATORY DATA ANALYSIS (EDA) </B></H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore Data statistics using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore Data Types using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore Data statistics ,schema & types using TensorFlow Data Validation (TFDV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfdv.generate_statistics_from_csv(f\"data/{city}/{city}_area.csv\")\n",
    "tfdv.visualize_statistics(data)\n",
    "schema = tfdv.infer_schema(statistics=data)\n",
    "tfdv.display_schema(schema=schema)\n",
    "# Check eval data for errors by validating the eval data stats using the previously inferred schema.\n",
    "anomalies = tfdv.validate_statistics(statistics=data, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorise fields into CATEGORICAL,NUMERICAL & to be DROPPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above there is a combination of categorical & numerical features with 'Price' being the label. We will need to classify the \n",
    "# features into the following:\n",
    "num_feat = ['Bedrooms','Bathrooms','Cars','Area','Latitude','Longitude','Distance','Price','Date']\n",
    "cat_feat = ['Suburb','Type','Method']\n",
    "drop_feat = ['Street','Address','State','Postcode','Agent']\n",
    "\n",
    "# Drop features that are not required\n",
    "if set(drop_feat).issubset(df.columns):\n",
    "  df = df.drop(drop_feat, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore Relationships between Numerical Features & Price (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = df[num_feat], height=3,diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for all the remaining numerical data including the target 'Price'\n",
    "# Define the heatmap parameters\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "# Define correlation matrix\n",
    "corr_matrix = df[num_feat].corr()\n",
    "\n",
    "# Replace correlation < |0.3| by 0 for a better visibility\n",
    "corr_matrix[(corr_matrix < 0.3) & (corr_matrix > -0.3)] = 0\n",
    "\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr_matrix, vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot_kws={\"size\": 9, \"color\": \"black\"},annot=True)\n",
    "plt.title(\"Price Correlation\")\n",
    "\n",
    "## Lets visualize individually \n",
    "\n",
    "corr =df.corr()[\"Price\"].sort_values(ascending = False)[1:len(num_feat)] ## selecting cols other than Saleprice, LogPrice\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Price Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = df.dtypes[df.dtypes != 'object'].index\n",
    "skew_feats = df[num_feat].skew().sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'skew': skew_feats})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> <B>FEATURE ENGINEERING & TRANSFORMATION </B></H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crafting new Feature YEAR from DATE\n",
    "2. Crafing new Feature 'LogPrice' from 'Price' to reduce significant skew present in 'Price'\n",
    "3. Transforming Feature 'Area' from 'Price' to reduce significant skew present in 'Price'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date']= df['Date'].astype('datetime64[ns]')\n",
    "df['Year'] = df['Date'].dt.year\n",
    "# df['PricePerSqm'] = df['Price']/df['Area']\n",
    "df[\"LogPrice\"] = np.log10(df[\"Price\"])\n",
    "# Added Area after I realised that Area has large skew, hence applying log10 to reduce Skew & make it more Gausian/normal distribution\n",
    "# then applu Z score as it works best on Normal/Gausian Distribution\n",
    "df['Area'] = np.log10(df['Area'])\n",
    "num_feat = ['Bedrooms','Bathrooms','Cars','Area','Latitude','Longitude','Distance','Year','LogPrice']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = ['Bedrooms','Bathrooms','Cars','Area','Latitude','Longitude','Distance','Year','LogPrice']\n",
    "#num_feat = df.dtypes[df.dtypes != 'object'].index\n",
    "skew_feats = df[num_feat].skew().sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'skew': skew_feats})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> <B> ANOMALY DETECTION & REMOVAL </B></H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out Outliers using either Z Score Method (normal discribution) or IQR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'Zscore'\n",
    "# Interquartile range (IQR) method should be used for NON normal distribution\n",
    "\n",
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out\n",
    "\n",
    "if method == 'Zscore':\n",
    "  #df = df[(np.abs(stats.zscore(df['Price'])) < 3)]\n",
    "  df = df[(np.abs(stats.zscore(df[num_feat])) < 3).all(axis=1)]\n",
    "  \n",
    "else:\n",
    "  df = remove_outlier(df, num_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore Data after removing anomalies & using Log10 on Price to reduce skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = df[num_feat], height=3,diag_kind='kde')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.displot(df['Price'])\n",
    "sns.displot(df['LogPrice'])\n",
    "\n",
    "df.describe()\n",
    "\n",
    "num_feat = df.dtypes[df.dtypes != 'object'].index\n",
    "skew_feats = df[num_feat].skew().sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'skew': skew_feats})\n",
    "skewness\n",
    "\n",
    "# Heatmap for all the remaining numerical data including the target 'Price'\n",
    "# Define the heatmap parameters\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "# Define correlation matrix\n",
    "corr_matrix = df[num_feat].corr()\n",
    "\n",
    "# Replace correlation < |0.3| by 0 for a better visibility\n",
    "corr_matrix[(corr_matrix < 0.3) & (corr_matrix > -0.3)] = 0\n",
    "\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr_matrix, vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot_kws={\"size\": 9, \"color\": \"black\"},annot=True)\n",
    "plt.title(\"Price Correlation\")\n",
    "\n",
    "## Lets visualize individually \n",
    "\n",
    "corr =df.corr()[\"Price\"].sort_values(ascending = False)[1:len(num_feat)] ## selecting cols other than Saleprice, LogPrice\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correlation matrix\n",
    "corr_matrix = df[num_feat].corr()\n",
    "\n",
    "# Replace correlation < |0.3| by 0 for a better visibility\n",
    "corr_matrix[(corr_matrix < 0.3) & (corr_matrix > -0.3)] = 0\n",
    "\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr_matrix, vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot_kws={\"size\": 9, \"color\": \"black\"},annot=True)\n",
    "plt.title(\"Price Correlation\")\n",
    "\n",
    "## Lets visualize individually \n",
    "\n",
    "corr =df.corr()[\"LogPrice\"].sort_values(ascending = False)[1:len(num_feat)] ## selecting cols other than Saleprice, LogPrice\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Skew of Area since it was really high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df['Area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that Outliers have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Data After reducing Skew of Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correlation matrix\n",
    "corr_matrix = df[num_feat].corr()\n",
    "\n",
    "# Replace correlation < |0.3| by 0 for a better visibility\n",
    "corr_matrix[(corr_matrix < 0.3) & (corr_matrix > -0.3)] = 0\n",
    "\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr_matrix, vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot_kws={\"size\": 9, \"color\": \"black\"},annot=True)\n",
    "plt.title(\"Price Correlation\")\n",
    "\n",
    "## Lets visualize individually \n",
    "\n",
    "corr =df.corr()[\"Price\"].sort_values(ascending = False)[1:len(num_feat)] ## selecting cols other than Saleprice, LogPrice\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = df.dtypes[df.dtypes != 'object'].index\n",
    "skew_feats = df[num_feat].skew().sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'skew': skew_feats})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4> Plot of Lattitude & longitude showing pricing in each location</H4>\n",
    "Observe that suburbs closer to the city centre generally are more expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = df['Longitude'], y = df['Latitude'],c =df['Price'],alpha=0.8,s=df['Price'],cmap='nipy_spectral' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Price','Date'], axis = 1)\n",
    "num_feat = ['Bedrooms','Bathrooms','Cars','Area','Latitude','Longitude','Distance','Year']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitting into TRAIN,VALIDATION & TEST DF using Scikit Learn function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the dataset into train, validation, and test sets as Pandas DF format\n",
    "train, test = train_test_split(df, test_size=0.2,random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2,random_state=42)\n",
    "\n",
    "Y_test = test['LogPrice']\n",
    "\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'get_scal' function takes a list of numerical features and returns a 'minmax' function\n",
    "# 'Minmax' function itself takes a 'numerical' number from a particular feature and return scaled value of that number.\n",
    "# Scalar def get_scal(feature):\n",
    "# TODO 1d\n",
    "def get_scal(feature):\n",
    "    def minmax(x):\n",
    "        mini = train[feature].min()\n",
    "        maxi = train[feature].max()\n",
    "        return (x - mini)/(maxi-mini)\n",
    "        return(minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# Numeric columns with Normalization\n",
    "for header in num_feat:\n",
    "    scal_input_fn = get_scal(header)\n",
    "    feature_columns.append(fc.numeric_column(header,normalizer_fn=scal_input_fn))\n",
    "\n",
    "# Categorical features with One Hot Encoding\n",
    "Type = fc.categorical_column_with_vocabulary_list('Type', df.Type.unique())\n",
    "Type_ohe = fc.indicator_column(Type)\n",
    "feature_columns.append(Type_ohe)\n",
    "\n",
    "Method = fc.categorical_column_with_vocabulary_list('Method', df.Method.unique())\n",
    "Method_ohe = fc.indicator_column(Method)\n",
    "feature_columns.append(Method_ohe)\n",
    "\n",
    "\n",
    "# Embedding categorical column with MANY unique values\n",
    "Suburb = fc.categorical_column_with_vocabulary_list('Suburb', df.Suburb.unique())\n",
    "# Commented out below & replaced with Suburb_ohe instead\n",
    "# Suburb_embedded = fc.embedding_column(Suburb, dimension=len(df.Suburb.unique()))\n",
    "Suburb_ohe = fc.indicator_column(Suburb)\n",
    "feature_columns.append(Suburb_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('LogPrice')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "\n",
    "batch_size = 32 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a normalization/scaling function to be used for numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the Training Data,Valuation & test Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Defining & Training a Tensorflow Sequential Regression DNN model.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model create\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "# `tf.keras.Sequential()` groups a linear stack of layers into a tf.keras.Model.\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(1, activation='linear',  name='Price')\n",
    "])\n",
    "\n",
    "# Model compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Model Fit\n",
    "history = model.fit(train_ds, validation_data = val_ds, epochs=60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Evaluating trained model performance using EVALUATION data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(val_ds)\n",
    "print(\"Mean Squared Error\", 10**mse)\n",
    "print(\"Root MEAN SQUARE ERROR $\",np.sqrt(10**mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> PLOTTING OF LOSS/MSE for TRAINING & EVALUATION DATA  </h4>\n",
    "Useful to determine underfitting/overfitting as well as optimal number of epcochs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use matplotlib to draw the model's loss curves for training and validation\n",
    "def plot_curves(history, metrics):\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for idx, key in enumerate(metrics):  \n",
    "        ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        plt.plot(history.history[key])\n",
    "        plt.plot(history.history['val_{}'.format(key)])\n",
    "        plt.title('model {}'.format(key))\n",
    "        plt.ylabel(key)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left');\n",
    "\n",
    "# Plotting\n",
    "plot_curves(history, ['loss', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> PREDICTION </H3>\n",
    "<h4>Using TEST data to make PRICE prediction & compare ACTUAL vs PREDICTED</h4>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = {'Suburb':'BAULKHAM HILLS','Type': 'House','Method': 'auction','Bedrooms':  4, 'Bathrooms':  2, 'Cars':  1, 'Area':  450,'Latitude':  -33.77157324083317 ,'Longitude':  150.98026592490677,'Distance':  26.5, 'Year':  2022}\n",
    "input_df = pd.DataFrame.from_dict(inp)\n",
    "inp_ds = tf.data.Dataset.from_tensor_slices((dict(input_df)))\n",
    "inp_ds = inp_ds.batch(1)\n",
    "\n",
    "pred = model.predict(inp_ds)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model10.predict(test_ds)\n",
    "print(predictions[1])\n",
    "\n",
    "test[\"Price\"] = round(10 ** test['LogPrice'])\n",
    "predictions = np.round(10 ** predictions)\n",
    "\n",
    "#\n",
    "for i in range(60):\n",
    "    diff = round((predictions[i][0] - test['Price'].iloc[i])/1000)\n",
    "    print('PREDICTION: ${0}'.format(predictions[i][0]) + '   ACTUAL: '+format(test['Price'].iloc[i]) + f' DIFFERENCE: ${diff}')\n",
    "\n",
    "\n",
    "x = range(0,4200000)\n",
    "y = x\n",
    "plt.scatter(test['Price'].iloc[0:len(predictions)],predictions[0:len(predictions)])\n",
    "plt.title(\"ACTUAL VS PREDICTED PRICE\")\n",
    "plt.xlabel(\"ACTUAL PRICE: $\")\n",
    "plt.ylabel(\"PREDICATED PRICE: $\")\n",
    "plt.plot(x,y,'red')\n",
    "\n",
    "print(\"ROOT MEAN SQUARE ERROR ON TEST DATA: $\",np.sqrt(mean_squared_error(test[\"Price\"],predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> SAVE MODEL </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vers = city_list.index(city)+1\n",
    "vers = 11\n",
    "model.save(f\"saved_models/{vers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = tf.keras.models.load_model('saved_models/8')\n",
    "model9 = tf.keras.models.load_model('saved_models/9')\n",
    "model10 = tf.keras.models.load_model('saved_models/10')\n",
    "model11 = tf.keras.models.load_model('saved_models/11')\n",
    "\n",
    "\n",
    "# Check its architecture\n",
    "# model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> SERVE MODEL</h4>\n",
    "<h5> RUN TF SERVING DOCKER CONTAINER </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker run -it -v /home/khaled/AUTOMATION-EXCEL\\:/tf_serving -p 8601:8601 --entrypoint /bin/bash tensorflow/serving\n",
    "#tensorflow_model_server --rest_api_port=8601 --model_name=melb_price --model_base_path=/tf_serving/saved_models/\n",
    "\n",
    "client = docker.from_env()\n",
    "container = client.containers.run(image = \"tensorflow/serving\", ports = {8601:8601},volumes = ['/home/khaled/MLrealestate:/tf_serving'], detach=True)\n",
    "container.exec_run('/bin/bash')\n",
    "print(container.exec_run('ls'))\n",
    "container.exec_run('tensorflow_model_server --rest_api_port=8601 --model_name=real_estate_price_est --model_base_path=/tf_serving/saved_models/',detach = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = examples['train_ds'].__iter__()\n",
    "next_element = iterator.get_next()\n",
    "pt = next_element[0]\n",
    "en = next_element[1]\n",
    "print(pt.numpy())\n",
    "print(en.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = list(inp.values())\n",
    "data = {\"instances\": [inp]}\n",
    "test.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> API CALL FOR PREDICTION</h4>\n",
    "<h5>  REST API POST for LOCAL TF SERVING CONTAINER - MODEL NAME: real_estate_est</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#curl -d '{\"instances\": [[0.25,0.07553956834532373,0.0,0.25,0.0,0.1937046004842615,0.9103448275862069,0.516068393160683,0.4633053471477789,1.0]]}' \\\n",
    "#-X POST http://localhost:8601/v1/models/melb_price:predict\n",
    "\n",
    "r = requests.post(url=\"http://localhost:8601/v1/models/real_estate_price_est:predict\", data=json.dumps(data))\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('data/{city}/{city}_area.csv')\n",
    "#len(df['Suburb'].unique())\n",
    "df = df.drop_duplicates(subset=['Suburb','Type','Method'], keep='last')\n",
    "df.to_csv('data/{city}/{city}_template.csv',index = False)\n",
    "df.info()\n",
    "#df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "heroku auth:login\n",
    "heroku container:login\n",
    "heroku container:push web -a tf-serve-model\n",
    "heroku container:release web -a tf-serve-model\n",
    "heroku logs -a tf-serve-model --tail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
